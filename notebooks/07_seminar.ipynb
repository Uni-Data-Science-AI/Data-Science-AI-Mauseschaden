{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47db24e2",
   "metadata": {},
   "source": [
    "Laden eines ZIP-Archiv mit dem Drohnen-Datensatz aus dem angegebenen GitHub-Repository und entpacken in das aktuelle Arbeitsverzeichnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de122ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o drones_dataset.zip https://github.com/SmartFarmingLab/drones_seminar_dataset/archive/refs/heads/main.zip\n",
    "!unzip drones_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bab3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0ba29",
   "metadata": {},
   "source": [
    "Logischer Split in train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1693cea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ data/tiles_2048/Weißenfelser Straße (250410PeO)/annotations_train.json gespeichert\n",
      "→ data/tiles_2048/Weißenfelser Straße (250410PeO)/annotations_val.json gespeichert\n",
      "→ data/tiles_2048/Weißenfelser Straße (250410PeO)/annotations_test.json gespeichert\n",
      "Fertig.\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "# === Pfade anpassen ===\n",
    "ANNOT_PATH = Path(\"data/tiles_2048/Weißenfelser Straße (250410PeO)/instances_all.json\")\n",
    "OUT_DIR = Path(\"data/tiles_2048/Weißenfelser Straße (250410PeO)\")\n",
    "\n",
    "# === Parameter ===\n",
    "SPLITS = {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1}\n",
    "RNG_SEED = 42\n",
    "\n",
    "# === JSON laden ===\n",
    "with open(ANNOT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "images = data[\"images\"]\n",
    "anns = data.get(\"annotations\", [])\n",
    "cats = data.get(\"categories\", [])\n",
    "\n",
    "# === Split zufällig berechnen ===\n",
    "random.seed(RNG_SEED)\n",
    "ids = [im[\"id\"] for im in images]\n",
    "random.shuffle(ids)\n",
    "n = len(ids)\n",
    "n_train = int(SPLITS[\"train\"] * n)\n",
    "n_val = int(SPLITS[\"val\"] * n)\n",
    "train_ids = set(ids[:n_train])\n",
    "val_ids   = set(ids[n_train:n_train+n_val])\n",
    "test_ids  = set(ids[n_train+n_val:])\n",
    "\n",
    "def subset(ids_set):\n",
    "    return {\n",
    "        \"images\": [im for im in images if im[\"id\"] in ids_set],\n",
    "        \"annotations\": [a for a in anns if a.get(\"image_id\") in ids_set],\n",
    "        \"categories\": cats\n",
    "    }\n",
    "\n",
    "# === Drei JSON-Dateien speichern ===\n",
    "for name, ids_set in [(\"train\", train_ids), (\"val\", val_ids), (\"test\", test_ids)]:\n",
    "    out_path = OUT_DIR / f\"annotations_{name}.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(subset(ids_set), f, ensure_ascii=False, indent=2)\n",
    "    print(f\"→ {out_path} gespeichert\")\n",
    "\n",
    "print(\"Fertig.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a171c9",
   "metadata": {},
   "source": [
    "\n",
    "Definiert die Klasse CocoDataset, die COCO-formatierte Datensätze (Bilder + Annotationen) lädt.\n",
    "\n",
    "Liest die JSON-Annotationen und ordnet sie den jeweiligen Bildern zu.\n",
    "\n",
    "Lädt Bilder, konvertiert sie in RGB und wendet optionale Transformationen an.\n",
    "\n",
    "Erstellt für jedes Bild ein Target-Dictionary mit image_id, boxes (Bounding Boxes) und labels.\n",
    "\n",
    "Gibt in __getitem__ ein Bild-Tensor und das zugehörige Target zurück.\n",
    "\n",
    "Definiert eine collate_fn, um Batches mit variabler Target-Länge korrekt zusammenzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b1d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from collections import defaultdict\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, images_dir, ann_path, transform=None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        with open(ann_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            d = json.load(f)\n",
    "        self.images = d[\"images\"]\n",
    "        self.anns = d.get(\"annotations\", [])\n",
    "        self.categories = d.get(\"categories\", [])\n",
    "        self.transform = transform or T.ToTensor()\n",
    "\n",
    "        # Index: image_id -> list(annotations)\n",
    "        by_img = defaultdict(list)\n",
    "        for a in self.anns:\n",
    "            by_img[a[\"image_id\"]].append(a)\n",
    "        self.by_img = by_img\n",
    "\n",
    "        # Indexliste\n",
    "        self.records = [(im[\"id\"], im[\"file_name\"]) for im in self.images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, file_name = self.records[idx]\n",
    "        img = Image.open(self.images_dir / file_name).convert(\"RGB\")\n",
    "\n",
    "        anns = self.by_img.get(image_id, [])\n",
    "        boxes, labels = [], []\n",
    "        for a in anns:\n",
    "            if \"bbox\" in a:  # COCO: [x, y, w, h]\n",
    "                x, y, w, h = a[\"bbox\"]\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "            if \"category_id\" in a:\n",
    "                labels.append(a[\"category_id\"])\n",
    "\n",
    "        target = {\n",
    "            \"image_id\": torch.tensor([image_id], dtype=torch.int64),\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,4), dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        img = self.transform(img)  # Tensor [C,H,W]\n",
    "        return img, target\n",
    "\n",
    "# Für variable Target-Längen (Detection) braucht der DataLoader ein einfaches collate_fn:\n",
    "def collate_fn(batch):\n",
    "    imgs, targets = list(zip(*batch))\n",
    "    return list(imgs), list(targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647160e",
   "metadata": {},
   "source": [
    "Definiert Pfade zu den Bild- und Annotationsdateien für Training, Validierung und Test.\n",
    "\n",
    "Erstellt für jede Datenteilmenge ein CocoDataset-Objekt.\n",
    "\n",
    "Initialisiert DataLoader für Training, Validierung und Test, um die Daten in Batches zu laden.\n",
    "\n",
    "Verwendet collate_fn, um variable Target-Größen (z. B. unterschiedliche Objektanzahlen pro Bild) korrekt zu verarbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99170ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIR = \"data/tiles_2048/Weißenfelser Straße (250410PeO)\"\n",
    "ANN_TRAIN = \"data/tiles_2048/Weißenfelser Straße (250410PeO)/annotations_train.json\"\n",
    "ANN_VAL   = \"data/tiles_2048/Weißenfelser Straße (250410PeO)/annotations_val.json\"\n",
    "ANN_TEST  = \"data/tiles_2048/Weißenfelser Straße (250410PeO)/annotations_test.json\"\n",
    "\n",
    "train_ds = CocoDataset(IMAGES_DIR, ANN_TRAIN)\n",
    "val_ds   = CocoDataset(IMAGES_DIR, ANN_VAL)\n",
    "test_ds  = CocoDataset(IMAGES_DIR, ANN_TEST)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,  num_workers=0, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df6a29",
   "metadata": {},
   "source": [
    "Initialisiert Faster R-CNN (ResNet50-FPN v2), ersetzt den Box-Kopf für num_classes_fg + 1 Klassen und verschiebt alles aufs passende Device.\n",
    "\n",
    "Richtet AdamW (LR=1e-4, Weight Decay 1e-4) und Mixed-Precision (AMP mit GradScaler) ein.\n",
    "\n",
    "Trainingsschleife über EPOCHS: lädt Batches, verschiebt Daten aufs Device, berechnet Loss im autocast-Kontext, führt Backprop mit skaliertem Loss aus und optimiert die Gewichte.\n",
    "\n",
    "Protokolliert Schritt- und Epochenverluste; misst Dauer pro Epoche.\n",
    "\n",
    "Speichert nach jeder Epoche die Modellgewichte unter models/model_epoch_{epoch}.pth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59438426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  21%|██        | 25/118 [13:50<51:30, 33.23s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m optimizer.zero_grad()\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Gesamten Verlust berechnen\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m loss = \u001b[38;5;28msum\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m.values())\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Backward-Pass mit skaliertem Verlust\u001b[39;00m\n\u001b[32m     42\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torchvision/models/detection/generalized_rcnn.py:105\u001b[39m, in \u001b[36mGeneralizedRCNN.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m    103\u001b[39m     features = OrderedDict([(\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m, features)])\n\u001b[32m    104\u001b[39m proposals, proposal_losses = \u001b[38;5;28mself\u001b[39m.rpn(images, features, targets)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m detections, detector_losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroi_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m detections = \u001b[38;5;28mself\u001b[39m.transform.postprocess(detections, images.image_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[32m    108\u001b[39m losses = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torchvision/models/detection/roi_heads.py:762\u001b[39m, in \u001b[36mRoIHeads.forward\u001b[39m\u001b[34m(self, features, proposals, image_shapes, targets)\u001b[39m\n\u001b[32m    759\u001b[39m     matched_idxs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    761\u001b[39m box_features = \u001b[38;5;28mself\u001b[39m.box_roi_pool(features, proposals, image_shapes)\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m box_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbox_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    763\u001b[39m class_logits, box_regression = \u001b[38;5;28mself\u001b[39m.box_predictor(box_features)\n\u001b[32m    765\u001b[39m result: List[Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torchvision_env/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m'\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(F.pad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode),\n\u001b[32m    454\u001b[39m                     weight, bias, \u001b[38;5;28mself\u001b[39m.stride,\n\u001b[32m    455\u001b[39m                     _pair(\u001b[32m0\u001b[39m), \u001b[38;5;28mself\u001b[39m.dilation, \u001b[38;5;28mself\u001b[39m.groups)\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\").to(device)\n",
    "\n",
    "in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, 2) # 1 Klasse + Hintergrund\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Adam-Optimierers mit Weight Decay\n",
    "# Fügt einen kleinen Strafterm hinzu, der große Gewichtswerte im Modell bestraft\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "EPOCHS = 10\n",
    "loss_per_epoch = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start = time.time()\n",
    "\n",
    "    # tqdm() zeigt Fortschrittsbalken an\n",
    "    for (imgs, targets) in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        # Alle Bilder im Batch auf GPU schieben\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        # Alle Targets im Batch auf GPU schieben\n",
    "        targets = [{k: v.to(device) for k,v in t.items()} for t in targets]\n",
    "        # Gradienten aller Modellparameter auf null zurücksetzen\n",
    "        optimizer.zero_grad()\n",
    "        # Gesamten Verlust berechnen\n",
    "        loss = sum(model(imgs, targets).values())\n",
    "        # Backward-Pass mit skaliertem Verlust\n",
    "        loss.backward()\n",
    "        # Modellparameter aktualisieren\n",
    "        optimizer.step()\n",
    "        # Skalierungsfaktor aktualisieren, damit kleine Gradienten nicht zu klein werden\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    loss_per_epoch.append(avg_loss)\n",
    "    dur = time.time() - start\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - train loss: {avg_loss:.4f} ({dur:.1f}s)\")\n",
    "\n",
    "# Modell speichern\n",
    "torch.save(model.state_dict(), \"last_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e29f7",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffff779",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_per_epoch, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss per epoch\")\n",
    "plt.savefig(\"metrics/loss_per_epoch.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f765e",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modell laden\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=None)\n",
    "in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, 2)\n",
    "model.load_state_dict(torch.load(\"models/best.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Kein Gradient nötig beim Evaluieren\n",
    "with torch.no_grad():\n",
    "    for imgs, targets in test_loader:\n",
    "        imgs = [im.to(device) for im in imgs]\n",
    "        preds = model(imgs)  # Liste aus Dictionaries [{boxes, labels, scores}, ...]\n",
    "\n",
    "        break  # Nur erste Batch (zum Anzeigen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab16d8b",
   "metadata": {},
   "source": [
    "Ergebnisse anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07122435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstes Bild + Vorhersage\n",
    "img = (imgs[0].cpu() * 255).byte()\n",
    "pred = preds[0]\n",
    "\n",
    "# Nur Boxen mit hoher Konfidenz zeigen\n",
    "keep = pred[\"scores\"] > 0.5\n",
    "boxes = pred[\"boxes\"][keep]\n",
    "labels = pred[\"labels\"][keep]\n",
    "\n",
    "# Bounding Boxes zeichnen\n",
    "img_boxed = draw_bounding_boxes(img, boxes, colors=\"red\", width=2)\n",
    "plt.imshow(F.to_pil_image(img_boxed))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchvision_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
