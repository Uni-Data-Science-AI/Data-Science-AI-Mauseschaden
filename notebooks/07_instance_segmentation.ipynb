{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ce371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 2: Dataloader für Instance Segmentation\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "class CocoInstSegDataset(Dataset):\n",
    "    def __init__(self, ann_file, img_root, resize_short=640):\n",
    "        self.ann = json.load(open(ann_file, \"r\"))\n",
    "        self.img_root = Path(img_root)\n",
    "        self.images = self.ann[\"images\"]\n",
    "        self.anns   = self.ann[\"annotations\"]\n",
    "        self.cats   = self.ann[\"categories\"]\n",
    "        self.resize_short = resize_short\n",
    "\n",
    "        # index: image_id -> list[ann]\n",
    "        self.by_img = {}\n",
    "        for a in self.anns:\n",
    "            self.by_img.setdefault(a[\"image_id\"], []).append(a)\n",
    "\n",
    "        # category_id -> 1..K (Foreground-Labels)\n",
    "        cat_ids = [c[\"id\"] for c in self.cats]\n",
    "        cat_ids_sorted = sorted(cat_ids)\n",
    "        self.cid_to_lbl = {cid: i + 1 for i, cid in enumerate(cat_ids_sorted)}\n",
    "        self.num_classes_fg = len(self.cid_to_lbl)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def _resize(self, img, boxes, segs):\n",
    "        \"\"\"Bild + Boxen + Segmente auf resize_short skalieren.\"\"\"\n",
    "        w0, h0 = img.size\n",
    "        if self.resize_short is None:\n",
    "            return img, boxes, segs, 1.0, 1.0\n",
    "\n",
    "        scale = self.resize_short / min(w0, h0)\n",
    "        new_w, new_h = int(round(w0 * scale)), int(round(h0 * scale))\n",
    "        img = img.resize((new_w, new_h), resample=Image.BILINEAR)\n",
    "\n",
    "        # Boxen skalieren\n",
    "        if boxes:\n",
    "            boxes = [\n",
    "                [\n",
    "                    b[0] * scale,\n",
    "                    b[1] * scale,\n",
    "                    b[2] * scale,\n",
    "                    b[3] * scale,\n",
    "                ]\n",
    "                for b in boxes\n",
    "            ]\n",
    "\n",
    "        # Segmente skalieren\n",
    "        new_segs = []\n",
    "        for seg in segs:\n",
    "            if not seg:\n",
    "                new_segs.append(seg)\n",
    "                continue\n",
    "            scaled = []\n",
    "            for s in seg:  # s ist eine flache Liste [x1,y1,x2,y2,...]\n",
    "                scaled_poly = []\n",
    "                for i in range(0, len(s), 2):\n",
    "                    x = s[i] * scale\n",
    "                    y = s[i + 1] * scale\n",
    "                    scaled_poly.extend([x, y])\n",
    "                scaled.append(scaled_poly)\n",
    "            new_segs.append(scaled)\n",
    "\n",
    "        return img, boxes, new_segs, scale, scale\n",
    "\n",
    "    def _polygons_to_mask(self, seg, h, w):\n",
    "        \"\"\"\n",
    "        seg: Liste von Polygonen, jedes Polygon = [x1,y1,x2,y2,...]\n",
    "        Rückgabe: HxW uint8-Maske (0/1)\n",
    "        \"\"\"\n",
    "        mask_img = Image.new(\"L\", (w, h), 0)\n",
    "        draw = ImageDraw.Draw(mask_img)\n",
    "        for poly in seg:\n",
    "            if len(poly) >= 6:\n",
    "                xy = [(poly[i], poly[i+1]) for i in range(0, len(poly), 2)]\n",
    "                draw.polygon(xy, outline=1, fill=1)\n",
    "        return np.array(mask_img, dtype=np.uint8)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        meta = self.images[idx]\n",
    "        img_path = self.img_root / meta[\"file_name\"]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w0, h0 = img.size\n",
    "\n",
    "        anns = self.by_img.get(meta[\"id\"], [])\n",
    "\n",
    "        # COCO-BBOX: [x, y, w, h] -> xyxy\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        segs = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for a in anns:\n",
    "            x, y, w, h = a[\"bbox\"]\n",
    "            xyxy = [x, y, x + w, y + h]\n",
    "            boxes.append(xyxy)\n",
    "            labels.append(self.cid_to_lbl[a[\"category_id\"]])\n",
    "            segs.append(a.get(\"segmentation\", []))\n",
    "            areas.append(a.get(\"area\", w * h))\n",
    "            iscrowd.append(a.get(\"iscrowd\", 0))\n",
    "\n",
    "        # Resizing\n",
    "        img, boxes, segs, sx, sy = self._resize(img, boxes, segs)\n",
    "        w, h = img.size\n",
    "\n",
    "        # Masken bauen\n",
    "        masks = []\n",
    "        for seg in segs:\n",
    "            if not seg:\n",
    "                masks.append(np.zeros((h, w), dtype=np.uint8))\n",
    "            else:\n",
    "                masks.append(self._polygons_to_mask(seg, h, w))\n",
    "\n",
    "        boxes_t = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        labels_t = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n",
    "        masks_t  = torch.tensor(np.stack(masks), dtype=torch.uint8) if masks else torch.zeros((0, h, w), dtype=torch.uint8)\n",
    "        areas_t  = torch.tensor(areas, dtype=torch.float32) if areas else torch.zeros((0,), dtype=torch.float32)\n",
    "        iscrowd_t = torch.tensor(iscrowd, dtype=torch.int64) if iscrowd else torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes_t,\n",
    "            \"labels\": labels_t,\n",
    "            \"masks\": masks_t,\n",
    "            \"image_id\": torch.tensor([meta[\"id\"]], dtype=torch.int64),\n",
    "            \"area\": areas_t,\n",
    "            \"iscrowd\": iscrowd_t,\n",
    "        }\n",
    "\n",
    "        img_t = F.to_tensor(img)  # [0..1], CxHxW\n",
    "        return img_t, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, targets = zip(*batch)\n",
    "    return list(imgs), list(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bba810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 3: Dataset & DataLoader\n",
    "\n",
    "ROOT = \"../final_dataset\"  # wie in deinem OD-Notebook\n",
    "\n",
    "train_ds = CocoInstSegDataset(\n",
    "    f\"{ROOT}/annotations/instances_train.json\",\n",
    "    f\"{ROOT}/images/train\",\n",
    "    resize_short=640,\n",
    ")\n",
    "val_ds = CocoInstSegDataset(\n",
    "    f\"{ROOT}/annotations/instances_val.json\",\n",
    "    f\"{ROOT}/images/val\",\n",
    "    resize_short=640,\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "num_classes_fg = train_ds.num_classes_fg  # Anzahl Vordergrund-Klassen aus COCO\n",
    "print(\"Foreground-Klassen:\", num_classes_fg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff6917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 4: Modell (Mask R-CNN)\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = maskrcnn_resnet50_fpn_v2(weights=\"DEFAULT\").to(device)\n",
    "\n",
    "# Klassifikations-Head anpassen\n",
    "in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes_fg + 1)\n",
    "\n",
    "# Masken-Head anpassen\n",
    "in_feats_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "    in_feats_mask, hidden_layer, num_classes_fg + 1\n",
    ")\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 6: Training (Instance Segmentation)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "EPOCHS = 40\n",
    "\n",
    "loss_per_step = []\n",
    "loss_per_epoch = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    n_steps = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [\n",
    "            {\n",
    "                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n",
    "                for k, v in t.items()\n",
    "            }\n",
    "            for t in targets\n",
    "        ]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss_dict.values())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total += loss.item()\n",
    "        n_steps += 1\n",
    "        loss_per_step.append(loss.item())\n",
    "\n",
    "    epoch_loss = total / max(1, n_steps)\n",
    "    loss_per_epoch.append(epoch_loss)\n",
    "    dur = time.time() - start\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - train loss: {epoch_loss:.4f} ({dur:.1f}s)\")\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), f\"models/maskrcnn_epoch_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 7: Trainingskurven\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.makedirs(\"metrics\", exist_ok=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_per_step)\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss per step\")\n",
    "plt.savefig(\"metrics/loss_per_step.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_per_epoch, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss per epoch\")\n",
    "plt.savefig(\"metrics/loss_per_epoch.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
